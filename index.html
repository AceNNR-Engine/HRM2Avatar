<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <!-- TODO: Replace with your paper title and author names -->
  <!-- <meta name="title" content="PAPER_TITLE - AUTHOR_NAMES"> -->
  <meta name="title" content="HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans - Shi, Jia, Liu, Zhang, Zhu, Yang, Ma, Niu, Lv">
  <!-- TODO: Write a compelling 150-160 character description of your research -->
  <meta name="description" content="We introduce HRM^2Avatar, a framework that produces photorealistic, personalized avatars from phone scans, supporting real-time rendering and animation on mobile devices. Combining dual-phase capture, a dynamically optimized clothed mesh-driven Gaussian representation, and an efficient GPU pipeline, it achieves both high visual fidelity and practical usability on consumer hardware.">
  <!-- TODO: Add 5-10 relevant keywords for your research area -->
  <meta name="keywords" content="Avatar, 3D Gaussian Splatting, Real-time Driving, Mobile Rendering">
  <!-- TODO: List all authors -->
  <meta name="author" content="Chao Shi, Shenghao Jia, Jinhui Liu, Yong Zhang, Liangchao Zhu, Zhonglei Yang, Jinze Ma, Chaoyue Niu, Chengfei Lv">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <!-- TODO: Replace with your institution or lab name -->
  <meta property="og:site_name" content="Alibaba Group and Shanghai Jiao Tong University">
  <!-- TODO: Same as paper title above -->
  <meta property="og:title" content="HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans">
  <!-- TODO: Same as description above -->
  <meta property="og:description" content="We introduce HRM^2Avatar, a framework that produces photorealistic, personalized avatars from phone scans, supporting real-time rendering and animation on mobile devices. Combining dual-phase capture, a dynamically optimized clothed mesh-driven Gaussian representation, and an efficient GPU pipeline, it achieves both high visual fidelity and practical usability on consumer hardware.">
  <!-- TODO: Replace with your actual website URL -->
  <meta property="og:url" content="https://acennr-engine.github.io/HRM2Avatar/">
  <!-- TODO: Create a 1200x630px preview image and update path -->
  <meta property="og:image" content="https://github.com/AceNNR-Engine/HRM2Avatar/blob/main/static/images/teaser.jpg">
  <meta property="og:image:width" content="7559">
  <meta property="og:image:height" content="5930">
  <meta property="og:image:alt" content="HRM^2Avatar">
  <meta property="article:published_time" content="2025-12-15T00:00:00.000Z">
  <meta property="article:author" content="Chao Shi and Shenghao Jia">
  <meta property="article:section" content="Computer Graphics">
  <meta property="article:tag" content="Avater">
  <meta property="article:tag" content="3D Gaussian Splatting">
  <meta property="article:tag" content="Real-time Driving">
  <meta property="article:tag" content="Mobile Rendering">

  <!-- Twitter, ÂÖàÂà†Êéâ‰∫Ü -->
  <!-- <meta name="twitter:card" content="summary_large_image"> -->
  <!-- TODO: Replace with your lab/institution Twitter handle -->
  <!-- <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE"> -->
  <!-- TODO: Replace with first author's Twitter handle -->
  <!-- <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE"> -->
  <!-- TODO: Same as paper title above -->
  <!-- <meta name="twitter:title" content="PAPER_TITLE"> -->
  <!-- TODO: Same as description above -->
  <!-- <meta name="twitter:description" content="BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS"> -->
  <!-- TODO: Same as social preview image above -->
  <!-- <meta name="twitter:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta name="twitter:image:alt" content="PAPER_TITLE - Research Preview"> -->

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans">
  <meta name="citation_author" content="Shi, Chao">
  <meta name="citation_author" content="Jia, Shenghao">
  <meta name="citation_author" content="Liu, Jinhui">
  <meta name="citation_author" content="Zhang, Yong">
  <meta name="citation_author" content="Zhu, Liangchao">
  <meta name="citation_author" content="Yang, Zhonglei">
  <meta name="citation_author" content="Ma, Jinze">
  <meta name="citation_author" content="Niu, Chaoyue">
  <meta name="citation_author" content="Lv, Chengfei">
  <meta name="citation_publication_date" content="2025">
  <meta name="citation_conference_title" content="SIGGRAPH Asia 2025">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf">
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">


  <!-- TODO: Replace with your paper title and authors -->
  <title>HRM^2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans - Shi, Jia, Liu, Zhang, Zhu, Yang, Ma, Niu, Lv | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link rel="apple-touch-icon" href="static/images/favicon.ico">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers ÂÖàÊ≥®Èáä‰∫ÜÔºåÂêéÈù¢‰ø°ÊÅØÂÆåÂñÑ‰∫ÜÂÜçÊõ¥Êñ∞Âêß-->
  <!-- <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "HRM2Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans",
    "description": "We introduce HRM2Avatar, a framework that produces photorealistic, personalized avatars from phone scans, supporting real-time rendering and animation on mobile devices. Combining dual-phase capture, a dynamically optimized clothed mesh-driven Gaussian representation, and an efficient GPU pipeline, it achieves both high visual fidelity and practical usability on consumer hardware.",
    "author": [
    {"@type":"Person","name":"Chao Shi","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Shenghao Jia","affiliation":{"@type":"Organization","name":"Shanghai Jiao Tong University"}},
    {"@type":"Person","name":"Jinhui Liu","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Yong Zhang","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Liangchao Zhu","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Zhonglei Yang","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Jinze Ma","affiliation":{"@type":"Organization","name":"Alibaba Group"}},
    {"@type":"Person","name":"Chaoyue Niu","affiliation":{"@type":"Organization","name":"Shanghai Jiao Tong University"}},
    {"@type":"Person","name":"Chengfei Lv","affiliation":{"@type":"Organization","name":"Alibaba Group"}}
    ],
    "datePublished": "2025-12-15",
    "publisher": {"@type":"Organization","name":"ACM"},
    "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
    "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
    "keywords": ["Avatar", "3D Gaussian Splatting", "Real-time Driving", "Mobile Rendering"],
    "abstract": "We present HRM^2Avatar, a novel framework for creating high-fidelity avatars from monocular phone scans, which can be rendered and animated in real-time on mobile devices. Monocular capture with commodity smartphones provides a low-cost, pervasive alternative to studio-grade multi-camera rigs, making avatar digitization accessible to non-expert users. Reconstructing high-fidelity avatars from single-view video sequences poses significant challenges due to deficient visual and geometric data relative to multi-camera setups. To address these limitations, at the data level, our method leverages two types of data captured with smartphones: static pose sequences for detailed texture reconstruction and dynamic motion sequences for learning pose-dependent deformations and lighting changes. At the representation level, we employ a lightweight yet expressive representation to reconstruct high-fidelity digital humans from sparse monocular data. First, we extract explicit garment meshes from monocular data to model clothing deformations more effectively. Second, we attach illumination-aware Gaussians to the mesh surface, enabling high-fidelity rendering and capturing pose-dependent lighting changes. This representation efficiently learns high-resolution and dynamic information from our tailored monocular data, enabling the creation of detailed avatars. At the rendering level, real-time performance is critical for rendering and animating high-fidelity avatars in AR/VR, social gaming, and on-device creation, demanding sub-frame responsiveness. Our fully GPU-driven rendering pipeline delivers 120 FPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution, over 2.7√ó faster than representative mobile-engine baselines. Experiments show that HRM2Avatar delivers superior visual realism and real-time interactivity at high resolutions, outperforming state-of-the-art monocular methods.",
    "citation": "BIBTEX_CITATION_HERE",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "RESEARCH_AREA_1"
      },
      {
        "@type": "Thing", 
        "name": "RESEARCH_AREA_2"
      }
    ]
  }
  </script> -->
  
  <!-- Website/Organization Structured Data -->
  <!-- <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "INSTITUTION_OR_LAB_NAME",
    "url": "https://YOUR_INSTITUTION_WEBSITE.com",
    "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
    "sameAs": [
      "https://twitter.com/YOUR_TWITTER_HANDLE",
      "https://github.com/YOUR_GITHUB_USERNAME"
    ]
  }
  </script> -->
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  
  <!-- More Works Dropdown -->
  <!-- <div class="more-works-container">
    <button class="more-works-btn" onclick="toggleMoreWorks()" title="View More Works from Our Lab">
      <i class="fas fa-flask"></i>
      More Works
      <i class="fas fa-chevron-down dropdown-arrow"></i>
    </button>
    <div class="more-works-dropdown" id="moreWorksDropdown">
      <div class="dropdown-header">
        <h4>More Works from Our Lab</h4>
        <button class="close-btn" onclick="toggleMoreWorks()">
          <i class="fas fa-times"></i>
        </button>
      </div>
      <div class="works-list">
        [TODO: Replace with your lab's related works]
        <a href="https://arxiv.org/abs/PAPER_ID_1" class="work-item" target="_blank">
          <div class="work-info">
            [TODO: Replace with actual paper title]
            <h5>Paper Title 1</h5>
            [TODO: Replace with brief description]
            <p>Brief description of the work and its main contribution.</p>
            [TODO: Replace with venue and year]
            <span class="work-venue">Conference/Journal 2024</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        [TODO: Add more related works or remove extra items]
        <a href="https://arxiv.org/abs/PAPER_ID_2" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 2</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
        <a href="https://arxiv.org/abs/PAPER_ID_3" class="work-item" target="_blank">
          <div class="work-info">
            <h5>Paper Title 3</h5>
            <p>Brief description of the work and its main contribution.</p>
            <span class="work-venue">Conference/Journal 2023</span>
          </div>
          <i class="fas fa-external-link-alt"></i>
        </a>
      </div>
    </div>
  </div> -->

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <!-- TODO: Replace with your paper title -->
            <h1 class="title is-1 publication-title">HRM<sup>2</sup>Avatar: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans</h1>
            <div class="is-size-5 publication-authors">
              <!-- TODO: Replace with your paper authors and their personal links -->
              <span class="author-block">
                <a target="_blank">Chao Shi</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Shenghao Jia</a><sup>1,2*</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jinhui Liu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Yong Zhang</a><sup>1&dagger;</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Liangchao Zhu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Zhonglei Yang</a><sup>1&Dagger;</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Jinze Ma</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Chaoyue Niu</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a target="_blank">Chengfei Lv</a><sup>1&dagger;</sup>
              </span>
            </div>

                  <div class="is-size-5 publication-authors">
                    <!-- TODO: Replace with your institution and conference/journal info -->
                    <span class="author-block"><sup>1</sup>Alibaba Group, <sup>2</sup>Shanghai Jiao Tong University<br>SIGGRAPH Asia 2025</span>
                    <!-- TODO: Remove this line if no equal contribution -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution, <sup>&dagger;</sup>Corresponding Author, <sup>&Dagger;</sup>Project Leader</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- TODO: Update with your arXiv paper ID -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2510.13587.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- TODO: Replace with your GitHub repository URL -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- TODO: Update with your arXiv paper ID -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2510.13587" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- TODO: Replace with your teaser video -->
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata"> -->
        <!-- TODO: Add your video file path here -->
        <!-- <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video> -->
      <img src="static/images/teaser.jpg" alt="teaser figure" loading="lazy"/>

      <!-- TODO: Replace with your video description -->
      <h2 class="subtitle has-text-centered">
        Our method creates high-fidelity avatars with realistic clothing dynamics by monocular smartphone scanning,
  and achieves 2048&times;945@120FPS on iPhone 15 Pro Max and 1920&times;1824x2@90FPS on Apple Vision Pro with 533,695 splats. 
  Each subject's data is captured using a single iPhone for 5 minutes.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
<!-- <section class="section hero is-white"> -->
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <!-- TODO: Replace with your paper abstract -->
          <p>
            We present HRM<sup>2</sup>Avatar, a novel framework for creating high-fidelity avatars from monocular phone scans, which can be rendered and animated in real-time on mobile devices.
Monocular capture with commodity smartphones provides a low-cost, pervasive alternative to studio-grade multi-camera rigs, making avatar digitization accessible to non-expert users.
Reconstructing high-fidelity avatars from single-view video sequences poses significant challenges due to deficient visual and geometric data relative to multi-camera setups.
To address these limitations, at the data level, our method leverages two types of data captured with smartphones: static pose sequences for detailed texture reconstruction and dynamic motion sequences for learning pose-dependent deformations and lighting changes.
At the representation level, we employ a lightweight yet expressive representation to reconstruct high-fidelity digital humans from sparse monocular data. 
First, we extract explicit garment meshes from monocular data to model clothing deformations more effectively. 
Second, we attach illumination-aware Gaussians to the mesh surface, enabling high-fidelity rendering and capturing pose-dependent lighting changes.
This representation efficiently learns high-resolution and dynamic information from our tailored monocular data, enabling the creation of detailed avatars.
At the rendering level, real-time performance is critical for rendering and animating high-fidelity avatars in AR/VR, social gaming, and on-device creation, demanding sub-frame responsiveness. Our fully GPU-driven rendering pipeline delivers 120 FPS on mobile devices and 90 FPS on standalone VR devices at 2K resolution, over 2.7&times; faster than representative mobile-engine baselines.
Experiments show that HRM<sup>2</sup>Avatar delivers superior visual realism and real-time interactivity at high resolutions, outperforming state-of-the-art monocular methods.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Method overview -->
<section class="hero teaser is-white">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- Êñ∞Â¢û Abstract Ê†áÈ¢ò -->
      <h2 class="title is-3" style="margin-top:1.5rem;">Method Overview</h2>

      <img src="static/images/overview.png" alt="teaser figure" loading="lazy"/>

      <h2 class="subtitle has-text-centered">
        Given the two-stage phone scans of a subject, we construct a clothed mesh-driven Gaussian avatar. 
        Static, texture-rich images impose stringent supervision on Gaussian attributes ùó¥ , while dynamic, 
        motion-intensive sequences prioritize optimization of deformation &#x0394;<b>V</b><sup><b>d</b></sup> and illumination <em>L</em>. 
        Through deformation MLP, illumination MLP and GPU-driven Gaussian rendering pipeline, real-time rendering and 
        animation of realistic avatars is achievable on mobile devices.
      </h2>
    </div>
  </div>
</section>
<!-- End Method overview -->


<!-- comparison -->
<section class="hero teaser is-light">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <!-- Êñ∞Â¢û Abstract Ê†áÈ¢ò -->
      <h2 class="title is-3" style="margin-top:1.5rem;">Comparisons</h2>

      <video poster="" id="tree" autoplay controls muted loop height="100%" preload="metadata">
        <!-- TODO: Add your video file path here -->
        <source src="static/videos/comparison.mp4" type="video/mp4">
      </video>
    </div>
  </div>
</section>
<!-- End comparison -->


<!-- Youtube video -->
<section class="hero is-small is-white">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- [TODO: Replace with your YouTube video ID] -->
            <iframe src="https://www.youtube.com/embed/y_iZ8_o0dNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->


<<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
      <button class="copy-bibtex-btn" onclick="copyBibTeX()" title="Copy BibTeX to clipboard">
        <i class="fas fa-copy"></i>
        <span class="copy-text">Copy</span>
      </button>
    </div>
    <pre id="bibtex-code"><code>@inproceedings{shi2025hrm2avatar,
  author = {Shi, Chao and Jia, Shenghao and Liu, Jinhui and Zhang, Yong and Zhu, Liangchao and Yang, Zhonglei and Ma, Jinze and Niu, Chaoyue and Lv, Chengfei},
  title     = {{HRM}$^2${Avatar}: High-Fidelity Real-Time Mobile Avatars from Monocular Phone Scans},
  booktitle = {SIGGRAPH Asia 2025 Conference Papers (SA Conference Papers '25)},
  year      = {2025},
  address   = {Hong Kong, Hong Kong},
  month     = dec,
  publisher = {Association for Computing Machinery},
  location  = {New York, NY, USA},
  numpages  = {12},
  doi       = {10.1145/3757377.3763894},
  url       = {https://doi.org/10.1145/3757377.3763894}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
